{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import CardDlayingDataset, final_transforms, custom_collate\n",
    "from model import CenterNet\n",
    "from loss import focal_loss, get_predictions_from_head\n",
    "from utils import print_base_img_annotated_img, plot_heatmaps, plot_loss_vs_epoch\n",
    "\n",
    "OVERFIT_BATCH = False\n",
    "arguments = sys.argv[1:]\n",
    "if len(arguments) and arguments[0] == 'overfit':\n",
    "    OVERFIT_BATCH = True\n",
    "\n",
    "\n",
    "    \n",
    "BASE_PATH = \"/Users/sacha.lahlou/Library/CloudStorage/OneDrive-PMU/centernetImplementation/centernet/cardDetectionDataset/\"\n",
    "TEST_PATH = \"test/\"\n",
    "TRAIN_PATH = \"train/\"\n",
    "VAL_PATH = \"valid/\"\n",
    "ANNOTATION_FILENAME = \"_annotations.coco.json\"\n",
    "\n",
    "MODEL_SAVE_PATH = './model_parameters_save'\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "INPUT_SHAPE = (3, 256, 256)\n",
    "NBCLASSES = 53\n",
    "\n",
    "size_loss_weight = 0.1\n",
    "offset_loss_weight = 1\n",
    "focal_loss_weight = 1\n",
    "\n",
    "l1loss = torch.nn.L1Loss()\n",
    "\n",
    "train_ds = CardDlayingDataset(BASE_PATH + TRAIN_PATH, transforms=final_transforms)\n",
    "test_ds = CardDlayingDataset(BASE_PATH + TEST_PATH, transforms=final_transforms)\n",
    "\n",
    "class_names = test_ds.classes_names\n",
    "train_data_len = train_ds.__len__()\n",
    "\n",
    "train_ds = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "test_ds = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "\n",
    "EPCH = 10\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print('running on device mps')\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    print('Warning: mps not available running on cpu')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = CenterNet(INPUT_SHAPE, NBCLASSES, need_fpn=False)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)#1e-3)\n",
    "\n",
    "\n",
    "print('\\n\\nStart training... \\n\\n')\n",
    "if OVERFIT_BATCH:\n",
    "    over_batch = 0\n",
    "    for batch in test_ds:\n",
    "        over_batch = batch\n",
    "        break\n",
    "#     print('overfitting batch...')\n",
    "#     for i in range(BATCH_SIZE):\n",
    "#         print_base_img_annotated_img(over_batch['img'][i], \n",
    "#                                  over_batch['labels'][i]['boxes'],\n",
    "#                                  over_batch['labels'][i]['labels'],\n",
    "#                                  class_names\n",
    "#                                  )\n",
    "\n",
    "\n",
    "def train_one_step(batch):\n",
    "    img =  batch['img'].to(device)\n",
    "    # labels =  batch['labels'].to(device)\n",
    "    hmaps = batch['hmaps'].to(device)\n",
    "    off_maps = batch['offset_map'].to(device)\n",
    "    size_maps = batch['size_map'].to(device)\n",
    "    center_position_save = batch['center_position_save'].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    hmaps_hat, off_maps_hat, size_map_hat = model(img)\n",
    "\n",
    "    hmaps_loss = focal_loss(hmaps_hat, hmaps) # torchvision.ops.sigmoid_focal_loss(hmaps_hat, hmaps, reduction='mean') \n",
    "\n",
    "    size_loss = l1loss(get_predictions_from_head(size_map_hat, center_position_save), size_maps)\n",
    "    off_loss = l1loss(get_predictions_from_head(off_maps_hat, center_position_save), off_maps)\n",
    "\n",
    "    loss_det = hmaps_loss * focal_loss_weight + size_loss * size_loss_weight + off_loss * offset_loss_weight\n",
    "\n",
    "    loss_det.backward()\n",
    "    optimizer.step()\n",
    "    return loss_det, hmaps_loss, size_loss, off_loss\n",
    "\n",
    "loss_list = []\n",
    "for epch_idx in range(EPCH):\n",
    "    if OVERFIT_BATCH:\n",
    "        FAKE_BATCH_SIZE = 120\n",
    "        for batch_idx in range(FAKE_BATCH_SIZE):\n",
    "            loss_det, hmaps_loss, size_loss, off_loss = train_one_step(over_batch)\n",
    "            loss_list.append([loss_det.item(), hmaps_loss.item(), size_loss.item(), off_loss.item()])\n",
    "            print(f'OVERFIT LOG: EPOCH: {epch_idx}/{EPCH} - BATCH: {batch_idx}/{FAKE_BATCH_SIZE} --> total loss: {loss_det} | focal_loss: {hmaps_loss}')\n",
    "    else:\n",
    "        for batch_idx, batch in enumerate(train_ds):\n",
    "            loss_det, hmaps_loss, size_loss, off_loss = train_one_step(batch)\n",
    "            loss_list.append([loss_det, hmaps_loss, size_loss, off_loss])\n",
    "            print(f'LOG: EPOCH: {epch_idx}/{EPCH} - BATCH: {batch_idx}/{train_data_len // BATCH_SIZE} --> total loss: {loss_det} | focal_loss: {hmaps_loss}')\n",
    "\n",
    "print('\\n\\nEnd training... \\n\\n')\n",
    "\n",
    "loss_list = np.array(loss_list).reshape(len(loss_list[0]), -1)\n",
    "plot_loss_vs_epoch(loss_list)\n",
    "\n",
    "print('---> ', torch.save(model.state_dict(), MODEL_SAVE_PATH))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
